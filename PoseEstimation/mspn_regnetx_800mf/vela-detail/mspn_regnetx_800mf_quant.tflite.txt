Warning: Unsupported TensorFlow Lite semantics for TRANSPOSE 'transpose_1'. Placing on CPU instead
 - Input(s), Output and Weight tensors must have quantization parameters
   Op has tensors with missing quantization parameters: transpose_1/perm
Warning: Unsupported TensorFlow Lite semantics for TRANSPOSE 'transpose_5'. Placing on CPU instead
 - Input(s), Output and Weight tensors must have quantization parameters
   Op has tensors with missing quantization parameters: transpose_101/perm
Warning: Unsupported TensorFlow Lite semantics for TRANSPOSE 'transpose_7'. Placing on CPU instead
 - Input(s), Output and Weight tensors must have quantization parameters
   Op has tensors with missing quantization parameters: transpose_1/perm
Warning: Unsupported TensorFlow Lite semantics for TRANSPOSE 'transpose_17'. Placing on CPU instead
 - Input(s), Output and Weight tensors must have quantization parameters
   Op has tensors with missing quantization parameters: transpose_101/perm
Warning: Unsupported TensorFlow Lite semantics for TRANSPOSE 'transpose_19'. Placing on CPU instead
 - Input(s), Output and Weight tensors must have quantization parameters
   Op has tensors with missing quantization parameters: transpose_1/perm
Warning: Unsupported TensorFlow Lite semantics for TRANSPOSE 'transpose_29'. Placing on CPU instead
 - Input(s), Output and Weight tensors must have quantization parameters
   Op has tensors with missing quantization parameters: transpose_101/perm
Warning: Unsupported TensorFlow Lite semantics for TRANSPOSE 'transpose_31'. Placing on CPU instead
 - Input(s), Output and Weight tensors must have quantization parameters
   Op has tensors with missing quantization parameters: transpose_1/perm
Warning: Unsupported TensorFlow Lite semantics for TRANSPOSE 'transpose_38'. Placing on CPU instead
 - Input(s), Output and Weight tensors must have quantization parameters
   Op has tensors with missing quantization parameters: transpose_101/perm
Warning: Unsupported TensorFlow Lite semantics for TRANSPOSE 'transpose_40'. Placing on CPU instead
 - Input(s), Output and Weight tensors must have quantization parameters
   Op has tensors with missing quantization parameters: transpose_1/perm
Warning: Unsupported TensorFlow Lite semantics for TRANSPOSE 'transpose_47'. Placing on CPU instead
 - Input(s), Output and Weight tensors must have quantization parameters
   Op has tensors with missing quantization parameters: transpose_101/perm
Warning: Unsupported TensorFlow Lite semantics for TRANSPOSE 'transpose_49'. Placing on CPU instead
 - Input(s), Output and Weight tensors must have quantization parameters
   Op has tensors with missing quantization parameters: transpose_1/perm
Warning: Unsupported TensorFlow Lite semantics for TRANSPOSE 'transpose_59'. Placing on CPU instead
 - Input(s), Output and Weight tensors must have quantization parameters
   Op has tensors with missing quantization parameters: transpose_101/perm
Warning: Unsupported TensorFlow Lite semantics for TRANSPOSE 'transpose_61'. Placing on CPU instead
 - Input(s), Output and Weight tensors must have quantization parameters
   Op has tensors with missing quantization parameters: transpose_1/perm
Warning: Unsupported TensorFlow Lite semantics for TRANSPOSE 'transpose_68'. Placing on CPU instead
 - Input(s), Output and Weight tensors must have quantization parameters
   Op has tensors with missing quantization parameters: transpose_101/perm
Warning: Unsupported TensorFlow Lite semantics for TRANSPOSE 'transpose_70'. Placing on CPU instead
 - Input(s), Output and Weight tensors must have quantization parameters
   Op has tensors with missing quantization parameters: transpose_1/perm
Warning: Unsupported TensorFlow Lite semantics for TRANSPOSE 'transpose_77'. Placing on CPU instead
 - Input(s), Output and Weight tensors must have quantization parameters
   Op has tensors with missing quantization parameters: transpose_101/perm
Warning: Unsupported TensorFlow Lite semantics for TRANSPOSE 'transpose_79'. Placing on CPU instead
 - Input(s), Output and Weight tensors must have quantization parameters
   Op has tensors with missing quantization parameters: transpose_1/perm
Warning: Unsupported TensorFlow Lite semantics for TRANSPOSE 'transpose_86'. Placing on CPU instead
 - Input(s), Output and Weight tensors must have quantization parameters
   Op has tensors with missing quantization parameters: transpose_101/perm
Warning: Unsupported TensorFlow Lite semantics for TRANSPOSE 'transpose_88'. Placing on CPU instead
 - Input(s), Output and Weight tensors must have quantization parameters
   Op has tensors with missing quantization parameters: transpose_1/perm
Warning: Unsupported TensorFlow Lite semantics for TRANSPOSE 'transpose_95'. Placing on CPU instead
 - Input(s), Output and Weight tensors must have quantization parameters
   Op has tensors with missing quantization parameters: transpose_101/perm
Warning: Unsupported TensorFlow Lite semantics for TRANSPOSE 'transpose_97'. Placing on CPU instead
 - Input(s), Output and Weight tensors must have quantization parameters
   Op has tensors with missing quantization parameters: transpose_1/perm
Warning: Unsupported TensorFlow Lite semantics for TRANSPOSE 'transpose_104'. Placing on CPU instead
 - Input(s), Output and Weight tensors must have quantization parameters
   Op has tensors with missing quantization parameters: transpose_101/perm
Warning: Unsupported TensorFlow Lite semantics for TRANSPOSE 'transpose_106'. Placing on CPU instead
 - Input(s), Output and Weight tensors must have quantization parameters
   Op has tensors with missing quantization parameters: transpose_1/perm
Warning: Unsupported TensorFlow Lite semantics for TRANSPOSE 'transpose_113'. Placing on CPU instead
 - Input(s), Output and Weight tensors must have quantization parameters
   Op has tensors with missing quantization parameters: transpose_101/perm
Warning: Unsupported TensorFlow Lite semantics for TRANSPOSE 'transpose_115'. Placing on CPU instead
 - Input(s), Output and Weight tensors must have quantization parameters
   Op has tensors with missing quantization parameters: transpose_1/perm
Warning: Unsupported TensorFlow Lite semantics for TRANSPOSE 'transpose_125'. Placing on CPU instead
 - Input(s), Output and Weight tensors must have quantization parameters
   Op has tensors with missing quantization parameters: transpose_101/perm
Warning: Unsupported TensorFlow Lite semantics for TRANSPOSE 'transpose_127'. Placing on CPU instead
 - Input(s), Output and Weight tensors must have quantization parameters
   Op has tensors with missing quantization parameters: transpose_1/perm
Warning: Unsupported TensorFlow Lite semantics for TRANSPOSE 'transpose_134'. Placing on CPU instead
 - Input(s), Output and Weight tensors must have quantization parameters
   Op has tensors with missing quantization parameters: transpose_101/perm
Warning: Unsupported TensorFlow Lite semantics for TRANSPOSE 'transpose_136'. Placing on CPU instead
 - Input(s), Output and Weight tensors must have quantization parameters
   Op has tensors with missing quantization parameters: transpose_1/perm
Warning: Unsupported TensorFlow Lite semantics for TRANSPOSE 'transpose_143'. Placing on CPU instead
 - Input(s), Output and Weight tensors must have quantization parameters
   Op has tensors with missing quantization parameters: transpose_101/perm
Warning: Unsupported TensorFlow Lite semantics for TRANSPOSE 'transpose_145'. Placing on CPU instead
 - Input(s), Output and Weight tensors must have quantization parameters
   Op has tensors with missing quantization parameters: transpose_1/perm
Warning: Unsupported TensorFlow Lite semantics for TRANSPOSE 'transpose_152'. Placing on CPU instead
 - Input(s), Output and Weight tensors must have quantization parameters
   Op has tensors with missing quantization parameters: transpose_101/perm
Warning: Unsupported TensorFlow Lite semantics for TRANSPOSE 'transpose_154'. Placing on CPU instead
 - Input(s), Output and Weight tensors must have quantization parameters
   Op has tensors with missing quantization parameters: transpose_1/perm
Warning: Unsupported TensorFlow Lite semantics for TRANSPOSE 'transpose_212'. Placing on CPU instead
 - Input(s), Output and Weight tensors must have quantization parameters
   Op has tensors with missing quantization parameters: transpose_101/perm
Warning: Unsupported TensorFlow Lite semantics for TRANSPOSE 'transpose_214'. Placing on CPU instead
 - Input(s), Output and Weight tensors must have quantization parameters
   Op has tensors with missing quantization parameters: transpose_1/perm
Warning: Unsupported TensorFlow Lite semantics for TRANSPOSE 'PartitionedCall:31'. Placing on CPU instead
 - Input(s), Output and Weight tensors must have quantization parameters
   Op has tensors with missing quantization parameters: transpose_101/perm
Warning: Unsupported TensorFlow Lite semantics for TRANSPOSE 'transpose_204'. Placing on CPU instead
 - Input(s), Output and Weight tensors must have quantization parameters
   Op has tensors with missing quantization parameters: transpose_101/perm
Warning: Unsupported TensorFlow Lite semantics for TRANSPOSE 'transpose_206'. Placing on CPU instead
 - Input(s), Output and Weight tensors must have quantization parameters
   Op has tensors with missing quantization parameters: transpose_1/perm
Warning: Unsupported TensorFlow Lite semantics for TRANSPOSE 'PartitionedCall:21'. Placing on CPU instead
 - Input(s), Output and Weight tensors must have quantization parameters
   Op has tensors with missing quantization parameters: transpose_101/perm
Warning: Unsupported TensorFlow Lite semantics for TRANSPOSE 'transpose_196'. Placing on CPU instead
 - Input(s), Output and Weight tensors must have quantization parameters
   Op has tensors with missing quantization parameters: transpose_101/perm
Warning: Unsupported TensorFlow Lite semantics for TRANSPOSE 'transpose_198'. Placing on CPU instead
 - Input(s), Output and Weight tensors must have quantization parameters
   Op has tensors with missing quantization parameters: transpose_1/perm
Warning: Unsupported TensorFlow Lite semantics for TRANSPOSE 'PartitionedCall:11'. Placing on CPU instead
 - Input(s), Output and Weight tensors must have quantization parameters
   Op has tensors with missing quantization parameters: transpose_101/perm
Warning: Unsupported TensorFlow Lite semantics for TRANSPOSE 'transpose_188'. Placing on CPU instead
 - Input(s), Output and Weight tensors must have quantization parameters
   Op has tensors with missing quantization parameters: transpose_101/perm
Warning: Unsupported TensorFlow Lite semantics for TRANSPOSE 'transpose_190'. Placing on CPU instead
 - Input(s), Output and Weight tensors must have quantization parameters
   Op has tensors with missing quantization parameters: transpose_1/perm
Warning: Unsupported TensorFlow Lite semantics for TRANSPOSE 'PartitionedCall:01'. Placing on CPU instead
 - Input(s), Output and Weight tensors must have quantization parameters
   Op has tensors with missing quantization parameters: transpose_101/perm
Warning: RESIZE_BILINEAR 'ResizeBilinear_2' is not supported on the NPU. Placing on CPU instead
 - The width and height of the IFM and OFM must match one of the following criteria:
        IFM W and H must both be 1
        IFM must match OFM
        W and H scaling must be equal and OFM W-1 and H-1 must be 2x/4x/8x IFM W-1 and H-1, if align_corners is True
        W and H scaling must be equal and OFM W and H must be 2x/4x/8x IFM W and H, if align_corners is False
   Op has ifm_shape=[1, 32, 24, 256], ofm_shape=[1, 64, 48, 256] and align_corners=True
Warning: RESIZE_BILINEAR 'ResizeBilinear_1' is not supported on the NPU. Placing on CPU instead
 - The width and height of the IFM and OFM must match one of the following criteria:
        IFM W and H must both be 1
        IFM must match OFM
        W and H scaling must be equal and OFM W-1 and H-1 must be 2x/4x/8x IFM W-1 and H-1, if align_corners is True
        W and H scaling must be equal and OFM W and H must be 2x/4x/8x IFM W and H, if align_corners is False
   Op has ifm_shape=[1, 16, 12, 256], ofm_shape=[1, 32, 24, 256] and align_corners=True
Warning: RESIZE_BILINEAR 'ResizeBilinear' is not supported on the NPU. Placing on CPU instead
 - The width and height of the IFM and OFM must match one of the following criteria:
        IFM W and H must both be 1
        IFM must match OFM
        W and H scaling must be equal and OFM W-1 and H-1 must be 2x/4x/8x IFM W-1 and H-1, if align_corners is True
        W and H scaling must be equal and OFM W and H must be 2x/4x/8x IFM W and H, if align_corners is False
   Op has ifm_shape=[1, 8, 6, 256], ofm_shape=[1, 16, 12, 256] and align_corners=True
Warning: RESIZE_BILINEAR 'ResizeBilinear_5' is not supported on the NPU. Placing on CPU instead
 - The width and height of the IFM and OFM must match one of the following criteria:
        IFM W and H must both be 1
        IFM must match OFM
        W and H scaling must be equal and OFM W-1 and H-1 must be 2x/4x/8x IFM W-1 and H-1, if align_corners is True
        W and H scaling must be equal and OFM W and H must be 2x/4x/8x IFM W and H, if align_corners is False
   Op has ifm_shape=[1, 32, 24, 17], ofm_shape=[1, 64, 48, 17] and align_corners=True
Warning: RESIZE_BILINEAR 'ResizeBilinear_4' is not supported on the NPU. Placing on CPU instead
 - The width and height of the IFM and OFM must match one of the following criteria:
        IFM W and H must both be 1
        IFM must match OFM
        W and H scaling must be equal and OFM W-1 and H-1 must be 2x/4x/8x IFM W-1 and H-1, if align_corners is True
        W and H scaling must be equal and OFM W and H must be 2x/4x/8x IFM W and H, if align_corners is False
   Op has ifm_shape=[1, 16, 12, 17], ofm_shape=[1, 64, 48, 17] and align_corners=True
Warning: RESIZE_BILINEAR 'ResizeBilinear_3' is not supported on the NPU. Placing on CPU instead
 - The width and height of the IFM and OFM must match one of the following criteria:
        IFM W and H must both be 1
        IFM must match OFM
        W and H scaling must be equal and OFM W-1 and H-1 must be 2x/4x/8x IFM W-1 and H-1, if align_corners is True
        W and H scaling must be equal and OFM W and H must be 2x/4x/8x IFM W and H, if align_corners is False
   Op has ifm_shape=[1, 8, 6, 17], ofm_shape=[1, 64, 48, 17] and align_corners=True
Warning: Transpose operation is unknown or unsupported, placing on CPU
Warning: Transpose operation is unknown or unsupported, placing on CPU
Warning: Transpose operation is unknown or unsupported, placing on CPU
Warning: ResizeBilinear operation is unknown or unsupported, placing on CPU
Warning: Transpose operation is unknown or unsupported, placing on CPU
Warning: ResizeBilinear operation is unknown or unsupported, placing on CPU
Warning: Transpose operation is unknown or unsupported, placing on CPU
Warning: Transpose operation is unknown or unsupported, placing on CPU
Warning: ResizeBilinear operation is unknown or unsupported, placing on CPU
Warning: Transpose operation is unknown or unsupported, placing on CPU
Warning: ResizeBilinear operation is unknown or unsupported, placing on CPU
Warning: Transpose operation is unknown or unsupported, placing on CPU
Warning: Transpose operation is unknown or unsupported, placing on CPU
Warning: ResizeBilinear operation is unknown or unsupported, placing on CPU
Warning: Transpose operation is unknown or unsupported, placing on CPU
Warning: ResizeBilinear operation is unknown or unsupported, placing on CPU
Warning: Transpose operation is unknown or unsupported, placing on CPU
Warning: Transpose operation is unknown or unsupported, placing on CPU
Warning: Transpose operation is unknown or unsupported, placing on CPU
Warning: Transpose operation is unknown or unsupported, placing on CPU
Warning: Transpose operation is unknown or unsupported, placing on CPU
Warning: Transpose operation is unknown or unsupported, placing on CPU
Warning: Transpose operation is unknown or unsupported, placing on CPU
Warning: Transpose operation is unknown or unsupported, placing on CPU
Warning: Transpose operation is unknown or unsupported, placing on CPU
Warning: Transpose operation is unknown or unsupported, placing on CPU
Warning: Transpose operation is unknown or unsupported, placing on CPU
Warning: Transpose operation is unknown or unsupported, placing on CPU
Warning: Transpose operation is unknown or unsupported, placing on CPU
Warning: Transpose operation is unknown or unsupported, placing on CPU
Warning: Transpose operation is unknown or unsupported, placing on CPU
Warning: Transpose operation is unknown or unsupported, placing on CPU
Warning: Transpose operation is unknown or unsupported, placing on CPU
Warning: Transpose operation is unknown or unsupported, placing on CPU
Warning: Transpose operation is unknown or unsupported, placing on CPU
Warning: Transpose operation is unknown or unsupported, placing on CPU
Warning: Transpose operation is unknown or unsupported, placing on CPU
Warning: Transpose operation is unknown or unsupported, placing on CPU
Warning: Transpose operation is unknown or unsupported, placing on CPU
Warning: Transpose operation is unknown or unsupported, placing on CPU
Warning: Transpose operation is unknown or unsupported, placing on CPU
Warning: Transpose operation is unknown or unsupported, placing on CPU
Warning: Transpose operation is unknown or unsupported, placing on CPU
Warning: Transpose operation is unknown or unsupported, placing on CPU
Warning: Transpose operation is unknown or unsupported, placing on CPU
Warning: Transpose operation is unknown or unsupported, placing on CPU
Warning: Transpose operation is unknown or unsupported, placing on CPU
Warning: Transpose operation is unknown or unsupported, placing on CPU
Warning: Transpose operation is unknown or unsupported, placing on CPU
Warning: Transpose operation is unknown or unsupported, placing on CPU
Warning: Transpose operation is unknown or unsupported, placing on CPU
<nng.Tensor 'Pad' shape=[1, 3, 258, 194] dtype=int8> adding consumer <nng.Operation 'transpose_1' type=Transpose>
<nng.Tensor 'transpose_1/perm' shape=[4] dtype=int32> adding consumer <nng.Operation 'transpose_1' type=Transpose>
<nng.Tensor 'transpose_1' shape=[1, 258, 194, 3] dtype=int8> adding consumer <nng.Operation 'call_main_split_2' type=CustomNpuOp>
<nng.Tensor 'onnx_tf_prefix_Relu_3;Add_1;convolution_6;convolution_1;Const_139' shape=[1, 128, 96, 64] dtype=int8> adding consumer <nng.Operation 'transpose_5' type=Transpose>
<nng.Tensor 'transpose_101/perm' shape=[4] dtype=int32> adding consumer <nng.Operation 'transpose_5' type=Transpose>
<nng.Tensor 'transpose_5' shape=[1, 64, 128, 96] dtype=int8> adding consumer <nng.Operation 'call_main_split_3' type=CustomNpuOp>
<nng.Tensor 'Pad_1' shape=[1, 64, 130, 98] dtype=int8> adding consumer <nng.Operation 'transpose_7' type=Transpose>
<nng.Tensor 'transpose_1/perm' shape=[4] dtype=int32> adding consumer <nng.Operation 'transpose_7' type=Transpose>
<nng.Tensor 'transpose_7' shape=[1, 130, 98, 64] dtype=int8> adding consumer <nng.Operation 'call_main_split_4' type=CustomNpuOp>
<nng.Tensor 'Add_4;convolution_6;convolution_7;Const_133' shape=[1, 64, 48, 64] dtype=int8> adding consumer <nng.Operation 'call_main_split_4' type=CustomNpuOp>
<nng.Tensor 'onnx_tf_prefix_Relu_11;Add_5;convolution_38;convolution_8;Const_131' shape=[1, 64, 48, 128] dtype=int8> adding consumer <nng.Operation 'transpose_17' type=Transpose>
<nng.Tensor 'transpose_101/perm' shape=[4] dtype=int32> adding consumer <nng.Operation 'transpose_17' type=Transpose>
<nng.Tensor 'transpose_17' shape=[1, 128, 64, 48] dtype=int8> adding consumer <nng.Operation 'call_main_split_5' type=CustomNpuOp>
<nng.Tensor 'Pad_2' shape=[1, 128, 66, 50] dtype=int8> adding consumer <nng.Operation 'transpose_19' type=Transpose>
<nng.Tensor 'transpose_1/perm' shape=[4] dtype=int32> adding consumer <nng.Operation 'transpose_19' type=Transpose>
<nng.Tensor 'transpose_19' shape=[1, 66, 50, 128] dtype=int8> adding consumer <nng.Operation 'call_main_split_6' type=CustomNpuOp>
<nng.Tensor 'Add_8;convolution_38;convolution_18;Const_125' shape=[1, 32, 24, 128] dtype=int8> adding consumer <nng.Operation 'call_main_split_6' type=CustomNpuOp>
<nng.Tensor 'onnx_tf_prefix_Relu_19;Add_9;convolution_38;convolution_19;Const_123' shape=[1, 32, 24, 128] dtype=int8> adding consumer <nng.Operation 'transpose_29' type=Transpose>
<nng.Tensor 'transpose_101/perm' shape=[4] dtype=int32> adding consumer <nng.Operation 'transpose_29' type=Transpose>
<nng.Tensor 'transpose_29' shape=[1, 128, 32, 24] dtype=int8> adding consumer <nng.Operation 'call_main_split_7' type=CustomNpuOp>
<nng.Tensor 'Pad_3' shape=[1, 128, 34, 26] dtype=int8> adding consumer <nng.Operation 'transpose_31' type=Transpose>
<nng.Tensor 'transpose_1/perm' shape=[4] dtype=int32> adding consumer <nng.Operation 'transpose_31' type=Transpose>
<nng.Tensor 'transpose_31' shape=[1, 34, 26, 128] dtype=int8> adding consumer <nng.Operation 'call_main_split_8' type=CustomNpuOp>
<nng.Tensor 'onnx_tf_prefix_Relu_17;onnx_tf_prefix_Add_16' shape=[1, 32, 24, 128] dtype=int8> adding consumer <nng.Operation 'call_main_split_8' type=CustomNpuOp>
<nng.Tensor 'onnx_tf_prefix_Relu_26;Add_12;convolution_38;convolution_29;Const_117' shape=[1, 32, 24, 128] dtype=int8> adding consumer <nng.Operation 'transpose_38' type=Transpose>
<nng.Tensor 'transpose_101/perm' shape=[4] dtype=int32> adding consumer <nng.Operation 'transpose_38' type=Transpose>
<nng.Tensor 'transpose_38' shape=[1, 128, 32, 24] dtype=int8> adding consumer <nng.Operation 'call_main_split_9' type=CustomNpuOp>
<nng.Tensor 'Pad_4' shape=[1, 128, 34, 26] dtype=int8> adding consumer <nng.Operation 'transpose_40' type=Transpose>
<nng.Tensor 'transpose_1/perm' shape=[4] dtype=int32> adding consumer <nng.Operation 'transpose_40' type=Transpose>
<nng.Tensor 'transpose_40' shape=[1, 34, 26, 128] dtype=int8> adding consumer <nng.Operation 'call_main_split_10' type=CustomNpuOp>
<nng.Tensor 'onnx_tf_prefix_Relu_24;onnx_tf_prefix_Add_23' shape=[1, 32, 24, 128] dtype=int8> adding consumer <nng.Operation 'call_main_split_10' type=CustomNpuOp>
<nng.Tensor 'onnx_tf_prefix_Relu_33;Add_15;convolution_179;convolution_39;Const_111' shape=[1, 32, 24, 288] dtype=int8> adding consumer <nng.Operation 'transpose_47' type=Transpose>
<nng.Tensor 'transpose_101/perm' shape=[4] dtype=int32> adding consumer <nng.Operation 'transpose_47' type=Transpose>
<nng.Tensor 'transpose_47' shape=[1, 288, 32, 24] dtype=int8> adding consumer <nng.Operation 'call_main_split_11' type=CustomNpuOp>
<nng.Tensor 'Pad_5' shape=[1, 288, 34, 26] dtype=int8> adding consumer <nng.Operation 'transpose_49' type=Transpose>
<nng.Tensor 'transpose_1/perm' shape=[4] dtype=int32> adding consumer <nng.Operation 'transpose_49' type=Transpose>
<nng.Tensor 'transpose_49' shape=[1, 34, 26, 288] dtype=int8> adding consumer <nng.Operation 'call_main_split_12' type=CustomNpuOp>
<nng.Tensor 'Add_18;convolution_179;convolution_59;Const_105' shape=[1, 16, 12, 288] dtype=int8> adding consumer <nng.Operation 'call_main_split_12' type=CustomNpuOp>
<nng.Tensor 'onnx_tf_prefix_Relu_41;Add_19;convolution_179;convolution_60;Const_103' shape=[1, 16, 12, 288] dtype=int8> adding consumer <nng.Operation 'transpose_59' type=Transpose>
<nng.Tensor 'transpose_101/perm' shape=[4] dtype=int32> adding consumer <nng.Operation 'transpose_59' type=Transpose>
<nng.Tensor 'transpose_59' shape=[1, 288, 16, 12] dtype=int8> adding consumer <nng.Operation 'call_main_split_13' type=CustomNpuOp>
<nng.Tensor 'Pad_6' shape=[1, 288, 18, 14] dtype=int8> adding consumer <nng.Operation 'transpose_61' type=Transpose>
<nng.Tensor 'transpose_1/perm' shape=[4] dtype=int32> adding consumer <nng.Operation 'transpose_61' type=Transpose>
<nng.Tensor 'transpose_61' shape=[1, 18, 14, 288] dtype=int8> adding consumer <nng.Operation 'call_main_split_14' type=CustomNpuOp>
<nng.Tensor 'onnx_tf_prefix_Relu_39;onnx_tf_prefix_Add_38' shape=[1, 16, 12, 288] dtype=int8> adding consumer <nng.Operation 'call_main_split_14' type=CustomNpuOp>
<nng.Tensor 'onnx_tf_prefix_Relu_48;Add_22;convolution_179;convolution_80;Const_97' shape=[1, 16, 12, 288] dtype=int8> adding consumer <nng.Operation 'transpose_68' type=Transpose>
<nng.Tensor 'transpose_101/perm' shape=[4] dtype=int32> adding consumer <nng.Operation 'transpose_68' type=Transpose>
<nng.Tensor 'transpose_68' shape=[1, 288, 16, 12] dtype=int8> adding consumer <nng.Operation 'call_main_split_15' type=CustomNpuOp>
<nng.Tensor 'Pad_7' shape=[1, 288, 18, 14] dtype=int8> adding consumer <nng.Operation 'transpose_70' type=Transpose>
<nng.Tensor 'transpose_1/perm' shape=[4] dtype=int32> adding consumer <nng.Operation 'transpose_70' type=Transpose>
<nng.Tensor 'transpose_70' shape=[1, 18, 14, 288] dtype=int8> adding consumer <nng.Operation 'call_main_split_16' type=CustomNpuOp>
<nng.Tensor 'onnx_tf_prefix_Relu_46;onnx_tf_prefix_Add_45' shape=[1, 16, 12, 288] dtype=int8> adding consumer <nng.Operation 'call_main_split_16' type=CustomNpuOp>
<nng.Tensor 'onnx_tf_prefix_Relu_55;Add_25;convolution_179;convolution_100;Const_91' shape=[1, 16, 12, 288] dtype=int8> adding consumer <nng.Operation 'transpose_77' type=Transpose>
<nng.Tensor 'transpose_101/perm' shape=[4] dtype=int32> adding consumer <nng.Operation 'transpose_77' type=Transpose>
<nng.Tensor 'transpose_77' shape=[1, 288, 16, 12] dtype=int8> adding consumer <nng.Operation 'call_main_split_17' type=CustomNpuOp>
<nng.Tensor 'Pad_8' shape=[1, 288, 18, 14] dtype=int8> adding consumer <nng.Operation 'transpose_79' type=Transpose>
<nng.Tensor 'transpose_1/perm' shape=[4] dtype=int32> adding consumer <nng.Operation 'transpose_79' type=Transpose>
<nng.Tensor 'transpose_79' shape=[1, 18, 14, 288] dtype=int8> adding consumer <nng.Operation 'call_main_split_18' type=CustomNpuOp>
<nng.Tensor 'onnx_tf_prefix_Relu_53;onnx_tf_prefix_Add_52' shape=[1, 16, 12, 288] dtype=int8> adding consumer <nng.Operation 'call_main_split_18' type=CustomNpuOp>
<nng.Tensor 'onnx_tf_prefix_Relu_62;Add_28;convolution_179;convolution_120;Const_85' shape=[1, 16, 12, 288] dtype=int8> adding consumer <nng.Operation 'transpose_86' type=Transpose>
<nng.Tensor 'transpose_101/perm' shape=[4] dtype=int32> adding consumer <nng.Operation 'transpose_86' type=Transpose>
<nng.Tensor 'transpose_86' shape=[1, 288, 16, 12] dtype=int8> adding consumer <nng.Operation 'call_main_split_19' type=CustomNpuOp>
<nng.Tensor 'Pad_9' shape=[1, 288, 18, 14] dtype=int8> adding consumer <nng.Operation 'transpose_88' type=Transpose>
<nng.Tensor 'transpose_1/perm' shape=[4] dtype=int32> adding consumer <nng.Operation 'transpose_88' type=Transpose>
<nng.Tensor 'transpose_88' shape=[1, 18, 14, 288] dtype=int8> adding consumer <nng.Operation 'call_main_split_20' type=CustomNpuOp>
<nng.Tensor 'onnx_tf_prefix_Relu_60;onnx_tf_prefix_Add_59' shape=[1, 16, 12, 288] dtype=int8> adding consumer <nng.Operation 'call_main_split_20' type=CustomNpuOp>
<nng.Tensor 'onnx_tf_prefix_Relu_69;Add_31;convolution_179;convolution_140;Const_79' shape=[1, 16, 12, 288] dtype=int8> adding consumer <nng.Operation 'transpose_95' type=Transpose>
<nng.Tensor 'transpose_101/perm' shape=[4] dtype=int32> adding consumer <nng.Operation 'transpose_95' type=Transpose>
<nng.Tensor 'transpose_95' shape=[1, 288, 16, 12] dtype=int8> adding consumer <nng.Operation 'call_main_split_21' type=CustomNpuOp>
<nng.Tensor 'Pad_10' shape=[1, 288, 18, 14] dtype=int8> adding consumer <nng.Operation 'transpose_97' type=Transpose>
<nng.Tensor 'transpose_1/perm' shape=[4] dtype=int32> adding consumer <nng.Operation 'transpose_97' type=Transpose>
<nng.Tensor 'transpose_97' shape=[1, 18, 14, 288] dtype=int8> adding consumer <nng.Operation 'call_main_split_22' type=CustomNpuOp>
<nng.Tensor 'onnx_tf_prefix_Relu_67;onnx_tf_prefix_Add_66' shape=[1, 16, 12, 288] dtype=int8> adding consumer <nng.Operation 'call_main_split_22' type=CustomNpuOp>
<nng.Tensor 'onnx_tf_prefix_Relu_76;Add_34;convolution_179;convolution_160;Const_73' shape=[1, 16, 12, 288] dtype=int8> adding consumer <nng.Operation 'transpose_104' type=Transpose>
<nng.Tensor 'transpose_101/perm' shape=[4] dtype=int32> adding consumer <nng.Operation 'transpose_104' type=Transpose>
<nng.Tensor 'transpose_104' shape=[1, 288, 16, 12] dtype=int8> adding consumer <nng.Operation 'call_main_split_23' type=CustomNpuOp>
<nng.Tensor 'Pad_11' shape=[1, 288, 18, 14] dtype=int8> adding consumer <nng.Operation 'transpose_106' type=Transpose>
<nng.Tensor 'transpose_1/perm' shape=[4] dtype=int32> adding consumer <nng.Operation 'transpose_106' type=Transpose>
<nng.Tensor 'transpose_106' shape=[1, 18, 14, 288] dtype=int8> adding consumer <nng.Operation 'call_main_split_24' type=CustomNpuOp>
<nng.Tensor 'onnx_tf_prefix_Relu_74;onnx_tf_prefix_Add_73' shape=[1, 16, 12, 288] dtype=int8> adding consumer <nng.Operation 'call_main_split_24' type=CustomNpuOp>
<nng.Tensor 'onnx_tf_prefix_Relu_83;Add_37;convolution_400;convolution_180;Const_67' shape=[1, 16, 12, 672] dtype=int8> adding consumer <nng.Operation 'transpose_113' type=Transpose>
<nng.Tensor 'transpose_101/perm' shape=[4] dtype=int32> adding consumer <nng.Operation 'transpose_113' type=Transpose>
<nng.Tensor 'transpose_113' shape=[1, 672, 16, 12] dtype=int8> adding consumer <nng.Operation 'call_main_split_25' type=CustomNpuOp>
<nng.Tensor 'Pad_12' shape=[1, 672, 18, 14] dtype=int8> adding consumer <nng.Operation 'transpose_115' type=Transpose>
<nng.Tensor 'transpose_1/perm' shape=[4] dtype=int32> adding consumer <nng.Operation 'transpose_115' type=Transpose>
<nng.Tensor 'transpose_115' shape=[1, 18, 14, 672] dtype=int8> adding consumer <nng.Operation 'call_main_split_26' type=CustomNpuOp>
<nng.Tensor 'Add_40;convolution_400;convolution_224;Const_61' shape=[1, 8, 6, 672] dtype=int8> adding consumer <nng.Operation 'call_main_split_26' type=CustomNpuOp>
<nng.Tensor 'onnx_tf_prefix_Relu_91;Add_41;convolution_400;convolution_225;Const_59' shape=[1, 8, 6, 672] dtype=int8> adding consumer <nng.Operation 'transpose_125' type=Transpose>
<nng.Tensor 'transpose_101/perm' shape=[4] dtype=int32> adding consumer <nng.Operation 'transpose_125' type=Transpose>
<nng.Tensor 'transpose_125' shape=[1, 672, 8, 6] dtype=int8> adding consumer <nng.Operation 'call_main_split_27' type=CustomNpuOp>
<nng.Tensor 'Pad_13' shape=[1, 672, 10, 8] dtype=int8> adding consumer <nng.Operation 'transpose_127' type=Transpose>
<nng.Tensor 'transpose_1/perm' shape=[4] dtype=int32> adding consumer <nng.Operation 'transpose_127' type=Transpose>
<nng.Tensor 'transpose_127' shape=[1, 10, 8, 672] dtype=int8> adding consumer <nng.Operation 'call_main_split_28' type=CustomNpuOp>
<nng.Tensor 'onnx_tf_prefix_Relu_89;onnx_tf_prefix_Add_88' shape=[1, 8, 6, 672] dtype=int8> adding consumer <nng.Operation 'call_main_split_28' type=CustomNpuOp>
<nng.Tensor 'onnx_tf_prefix_Relu_98;Add_44;convolution_400;convolution_269;Const_53' shape=[1, 8, 6, 672] dtype=int8> adding consumer <nng.Operation 'transpose_134' type=Transpose>
<nng.Tensor 'transpose_101/perm' shape=[4] dtype=int32> adding consumer <nng.Operation 'transpose_134' type=Transpose>
<nng.Tensor 'transpose_134' shape=[1, 672, 8, 6] dtype=int8> adding consumer <nng.Operation 'call_main_split_29' type=CustomNpuOp>
<nng.Tensor 'Pad_14' shape=[1, 672, 10, 8] dtype=int8> adding consumer <nng.Operation 'transpose_136' type=Transpose>
<nng.Tensor 'transpose_1/perm' shape=[4] dtype=int32> adding consumer <nng.Operation 'transpose_136' type=Transpose>
<nng.Tensor 'transpose_136' shape=[1, 10, 8, 672] dtype=int8> adding consumer <nng.Operation 'call_main_split_30' type=CustomNpuOp>
<nng.Tensor 'onnx_tf_prefix_Relu_96;onnx_tf_prefix_Add_95' shape=[1, 8, 6, 672] dtype=int8> adding consumer <nng.Operation 'call_main_split_30' type=CustomNpuOp>
<nng.Tensor 'onnx_tf_prefix_Relu_105;Add_47;convolution_400;convolution_313;Const_47' shape=[1, 8, 6, 672] dtype=int8> adding consumer <nng.Operation 'transpose_143' type=Transpose>
<nng.Tensor 'transpose_101/perm' shape=[4] dtype=int32> adding consumer <nng.Operation 'transpose_143' type=Transpose>
<nng.Tensor 'transpose_143' shape=[1, 672, 8, 6] dtype=int8> adding consumer <nng.Operation 'call_main_split_31' type=CustomNpuOp>
<nng.Tensor 'Pad_15' shape=[1, 672, 10, 8] dtype=int8> adding consumer <nng.Operation 'transpose_145' type=Transpose>
<nng.Tensor 'transpose_1/perm' shape=[4] dtype=int32> adding consumer <nng.Operation 'transpose_145' type=Transpose>
<nng.Tensor 'transpose_145' shape=[1, 10, 8, 672] dtype=int8> adding consumer <nng.Operation 'call_main_split_32' type=CustomNpuOp>
<nng.Tensor 'onnx_tf_prefix_Relu_103;onnx_tf_prefix_Add_102' shape=[1, 8, 6, 672] dtype=int8> adding consumer <nng.Operation 'call_main_split_32' type=CustomNpuOp>
<nng.Tensor 'onnx_tf_prefix_Relu_112;Add_50;convolution_400;convolution_357;Const_41' shape=[1, 8, 6, 672] dtype=int8> adding consumer <nng.Operation 'transpose_152' type=Transpose>
<nng.Tensor 'transpose_101/perm' shape=[4] dtype=int32> adding consumer <nng.Operation 'transpose_152' type=Transpose>
<nng.Tensor 'transpose_152' shape=[1, 672, 8, 6] dtype=int8> adding consumer <nng.Operation 'call_main_split_33' type=CustomNpuOp>
<nng.Tensor 'Pad_16' shape=[1, 672, 10, 8] dtype=int8> adding consumer <nng.Operation 'transpose_154' type=Transpose>
<nng.Tensor 'transpose_1/perm' shape=[4] dtype=int32> adding consumer <nng.Operation 'transpose_154' type=Transpose>
<nng.Tensor 'transpose_154' shape=[1, 10, 8, 672] dtype=int8> adding consumer <nng.Operation 'call_main_split_34' type=CustomNpuOp>
<nng.Tensor 'onnx_tf_prefix_Relu_110;onnx_tf_prefix_Add_109' shape=[1, 8, 6, 672] dtype=int8> adding consumer <nng.Operation 'call_main_split_34' type=CustomNpuOp>
<nng.Tensor 'onnx_tf_prefix_Relu_160;Add_60;convolution_414;convolution_408;Const_18' shape=[1, 8, 6, 256] dtype=int8> adding consumer <nng.Operation 'transpose_188' type=Transpose>
<nng.Tensor 'transpose_101/perm' shape=[4] dtype=int32> adding consumer <nng.Operation 'transpose_188' type=Transpose>
<nng.Tensor 'transpose_188' shape=[1, 256, 8, 6] dtype=int8> adding consumer <nng.Operation 'call_main_split_35' type=CustomNpuOp>
<nng.Tensor 'onnx_tf_prefix_Relu_119;Add_53;convolution_414;convolution_401;Const_35' shape=[1, 8, 6, 256] dtype=int8> adding consumer <nng.Operation 'ResizeBilinear' type=ResizeBilinear>
<nng.Tensor 'Cast' shape=[2] dtype=int32> adding consumer <nng.Operation 'ResizeBilinear' type=ResizeBilinear>
<nng.Tensor 'ResizeBilinear' shape=[1, 16, 12, 256] dtype=int8> adding consumer <nng.Operation 'call_main_split_38' type=CustomNpuOp>
<nng.Tensor 'onnx_tf_prefix_Relu_81;onnx_tf_prefix_Add_80' shape=[1, 16, 12, 288] dtype=int8> adding consumer <nng.Operation 'call_main_split_38' type=CustomNpuOp>
<nng.Tensor 'onnx_tf_prefix_Relu_172;Add_62;convolution_414;convolution_410;Const_13' shape=[1, 16, 12, 256] dtype=int8> adding consumer <nng.Operation 'transpose_196' type=Transpose>
<nng.Tensor 'transpose_101/perm' shape=[4] dtype=int32> adding consumer <nng.Operation 'transpose_196' type=Transpose>
<nng.Tensor 'transpose_196' shape=[1, 256, 16, 12] dtype=int8> adding consumer <nng.Operation 'call_main_split_39' type=CustomNpuOp>
<nng.Tensor 'onnx_tf_prefix_Relu_132;onnx_tf_prefix_Add_131' shape=[1, 16, 12, 256] dtype=int8> adding consumer <nng.Operation 'ResizeBilinear_1' type=ResizeBilinear>
<nng.Tensor 'Cast_1' shape=[2] dtype=int32> adding consumer <nng.Operation 'ResizeBilinear_1' type=ResizeBilinear>
<nng.Tensor 'ResizeBilinear_1' shape=[1, 32, 24, 256] dtype=int8> adding consumer <nng.Operation 'call_main_split_42' type=CustomNpuOp>
<nng.Tensor 'onnx_tf_prefix_Relu_31;onnx_tf_prefix_Add_30' shape=[1, 32, 24, 128] dtype=int8> adding consumer <nng.Operation 'call_main_split_42' type=CustomNpuOp>
<nng.Tensor 'onnx_tf_prefix_Relu_184;Add_64;convolution_414;convolution_412;Const_8' shape=[1, 32, 24, 256] dtype=int8> adding consumer <nng.Operation 'transpose_204' type=Transpose>
<nng.Tensor 'transpose_101/perm' shape=[4] dtype=int32> adding consumer <nng.Operation 'transpose_204' type=Transpose>
<nng.Tensor 'transpose_204' shape=[1, 256, 32, 24] dtype=int8> adding consumer <nng.Operation 'call_main_split_43' type=CustomNpuOp>
<nng.Tensor 'onnx_tf_prefix_Relu_145;onnx_tf_prefix_Add_144' shape=[1, 32, 24, 256] dtype=int8> adding consumer <nng.Operation 'ResizeBilinear_2' type=ResizeBilinear>
<nng.Tensor 'Cast_2' shape=[2] dtype=int32> adding consumer <nng.Operation 'ResizeBilinear_2' type=ResizeBilinear>
<nng.Tensor 'ResizeBilinear_2' shape=[1, 64, 48, 256] dtype=int8> adding consumer <nng.Operation 'call_main_split_46' type=CustomNpuOp>
<nng.Tensor 'onnx_tf_prefix_Relu_9;onnx_tf_prefix_Add_8' shape=[1, 64, 48, 64] dtype=int8> adding consumer <nng.Operation 'call_main_split_46' type=CustomNpuOp>
<nng.Tensor 'onnx_tf_prefix_Relu_196;Add_66;convolution_414;Const_3' shape=[1, 64, 48, 256] dtype=int8> adding consumer <nng.Operation 'transpose_212' type=Transpose>
<nng.Tensor 'transpose_101/perm' shape=[4] dtype=int32> adding consumer <nng.Operation 'transpose_212' type=Transpose>
<nng.Tensor 'transpose_212' shape=[1, 256, 64, 48] dtype=int8> adding consumer <nng.Operation 'call_main_split_47' type=CustomNpuOp>
<nng.Tensor 'transpose_5_npu' shape=[1, 64, 128, 96] dtype=int8> adding consumer <nng.Operation 'Pad_1_concat1_avgpool' type=AvgPool>
<nng.Tensor 'Pad_1_right_0_npu' shape=[1, 64, 128, 1] dtype=int8> adding consumer <nng.Operation 'Pad_1_concat2_avgpool' type=AvgPool>
<nng.Tensor 'transpose_17_npu' shape=[1, 128, 64, 48] dtype=int8> adding consumer <nng.Operation 'Pad_2_concat1_avgpool' type=AvgPool>
<nng.Tensor 'Pad_2_right_0_npu' shape=[1, 128, 64, 1] dtype=int8> adding consumer <nng.Operation 'Pad_2_concat2_avgpool' type=AvgPool>
<nng.Tensor 'transpose_29_npu' shape=[1, 128, 32, 24] dtype=int8> adding consumer <nng.Operation 'Pad_3_concat1_avgpool' type=AvgPool>
<nng.Tensor 'Pad_3_right_0_npu' shape=[1, 128, 32, 1] dtype=int8> adding consumer <nng.Operation 'Pad_3_concat2_avgpool' type=AvgPool>
<nng.Tensor 'transpose_38_npu' shape=[1, 128, 32, 24] dtype=int8> adding consumer <nng.Operation 'Pad_4_concat1_avgpool' type=AvgPool>
<nng.Tensor 'Pad_4_right_0_npu' shape=[1, 128, 32, 1] dtype=int8> adding consumer <nng.Operation 'Pad_4_concat2_avgpool' type=AvgPool>
<nng.Tensor 'transpose_47_npu' shape=[1, 288, 32, 24] dtype=int8> adding consumer <nng.Operation 'Pad_5_concat1_avgpool' type=AvgPool>
<nng.Tensor 'Pad_5_right_0_npu' shape=[1, 288, 32, 1] dtype=int8> adding consumer <nng.Operation 'Pad_5_concat2_avgpool' type=AvgPool>
<nng.Tensor 'transpose_59_npu' shape=[1, 288, 16, 12] dtype=int8> adding consumer <nng.Operation 'Pad_6_concat1_avgpool' type=AvgPool>
<nng.Tensor 'Pad_6_right_0_npu' shape=[1, 288, 16, 1] dtype=int8> adding consumer <nng.Operation 'Pad_6_concat2_avgpool' type=AvgPool>
<nng.Tensor 'transpose_68_npu' shape=[1, 288, 16, 12] dtype=int8> adding consumer <nng.Operation 'Pad_7_concat1_avgpool' type=AvgPool>
<nng.Tensor 'Pad_7_right_0_npu' shape=[1, 288, 16, 1] dtype=int8> adding consumer <nng.Operation 'Pad_7_concat2_avgpool' type=AvgPool>
<nng.Tensor 'transpose_77_npu' shape=[1, 288, 16, 12] dtype=int8> adding consumer <nng.Operation 'Pad_8_concat1_avgpool' type=AvgPool>
<nng.Tensor 'Pad_8_right_0_npu' shape=[1, 288, 16, 1] dtype=int8> adding consumer <nng.Operation 'Pad_8_concat2_avgpool' type=AvgPool>
<nng.Tensor 'transpose_86_npu' shape=[1, 288, 16, 12] dtype=int8> adding consumer <nng.Operation 'Pad_9_concat1_avgpool' type=AvgPool>
<nng.Tensor 'Pad_9_right_0_npu' shape=[1, 288, 16, 1] dtype=int8> adding consumer <nng.Operation 'Pad_9_concat2_avgpool' type=AvgPool>
<nng.Tensor 'transpose_95_npu' shape=[1, 288, 16, 12] dtype=int8> adding consumer <nng.Operation 'Pad_10_concat1_avgpool' type=AvgPool>
<nng.Tensor 'Pad_10_right_0_npu' shape=[1, 288, 16, 1] dtype=int8> adding consumer <nng.Operation 'Pad_10_concat2_avgpool' type=AvgPool>
<nng.Tensor 'transpose_104_npu' shape=[1, 288, 16, 12] dtype=int8> adding consumer <nng.Operation 'Pad_11_concat1_avgpool' type=AvgPool>
<nng.Tensor 'Pad_11_right_0_npu' shape=[1, 288, 16, 1] dtype=int8> adding consumer <nng.Operation 'Pad_11_concat2_avgpool' type=AvgPool>
<nng.Tensor 'transpose_113_npu' shape=[1, 672, 16, 12] dtype=int8> adding consumer <nng.Operation 'Pad_12_concat1_avgpool' type=AvgPool>
<nng.Tensor 'Pad_12_right_0_npu' shape=[1, 672, 16, 1] dtype=int8> adding consumer <nng.Operation 'Pad_12_concat2_avgpool' type=AvgPool>
<nng.Tensor 'transpose_125_npu' shape=[1, 672, 8, 6] dtype=int8> adding consumer <nng.Operation 'Pad_13_concat1_avgpool' type=AvgPool>
<nng.Tensor 'Pad_13_right_0_npu' shape=[1, 672, 8, 1] dtype=int8> adding consumer <nng.Operation 'Pad_13_concat2_avgpool' type=AvgPool>
<nng.Tensor 'transpose_134_npu' shape=[1, 672, 8, 6] dtype=int8> adding consumer <nng.Operation 'Pad_14_concat1_avgpool' type=AvgPool>
<nng.Tensor 'Pad_14_right_0_npu' shape=[1, 672, 8, 1] dtype=int8> adding consumer <nng.Operation 'Pad_14_concat2_avgpool' type=AvgPool>
<nng.Tensor 'transpose_143_npu' shape=[1, 672, 8, 6] dtype=int8> adding consumer <nng.Operation 'Pad_15_concat1_avgpool' type=AvgPool>
<nng.Tensor 'Pad_15_right_0_npu' shape=[1, 672, 8, 1] dtype=int8> adding consumer <nng.Operation 'Pad_15_concat2_avgpool' type=AvgPool>
<nng.Tensor 'transpose_152_npu' shape=[1, 672, 8, 6] dtype=int8> adding consumer <nng.Operation 'Pad_16_concat1_avgpool' type=AvgPool>
<nng.Tensor 'Pad_16_right_0_npu' shape=[1, 672, 8, 1] dtype=int8> adding consumer <nng.Operation 'Pad_16_concat2_avgpool' type=AvgPool>
<nng.Tensor 'transpose_188_npu' shape=[1, 256, 8, 6] dtype=int8> adding consumer <nng.Operation 'Pad_17_concat1_avgpool' type=AvgPool>
<nng.Tensor 'Pad_17_right_0_npu' shape=[1, 256, 8, 1] dtype=int8> adding consumer <nng.Operation 'Pad_17_concat2_avgpool' type=AvgPool>
<nng.Tensor 'transpose_196_npu' shape=[1, 256, 16, 12] dtype=int8> adding consumer <nng.Operation 'Pad_18_concat1_avgpool' type=AvgPool>
<nng.Tensor 'Pad_18_right_0_npu' shape=[1, 256, 16, 1] dtype=int8> adding consumer <nng.Operation 'Pad_18_concat2_avgpool' type=AvgPool>
<nng.Tensor 'transpose_204_npu' shape=[1, 256, 32, 24] dtype=int8> adding consumer <nng.Operation 'Pad_19_concat1_avgpool' type=AvgPool>
<nng.Tensor 'Pad_19_right_0_npu' shape=[1, 256, 32, 1] dtype=int8> adding consumer <nng.Operation 'Pad_19_concat2_avgpool' type=AvgPool>
<nng.Tensor 'transpose_212_npu' shape=[1, 256, 64, 48] dtype=int8> adding consumer <nng.Operation 'Pad_20_concat1_avgpool' type=AvgPool>
<nng.Tensor 'Pad_20_right_0_npu' shape=[1, 256, 64, 1] dtype=int8> adding consumer <nng.Operation 'Pad_20_concat2_avgpool' type=AvgPool>

Network summary for mspn_regnetx_800mf_quant
Accelerator configuration               Ethos_U65_256
System configuration                 internal-default
Memory mode                          internal-default
Accelerator clock                                1000 MHz
Design peak SRAM bandwidth                      16.00 GB/s
Design peak DRAM bandwidth                       3.75 GB/s

Total SRAM used                                316.94 KiB
Total DRAM used                               7931.20 KiB

CPU operators = 11 (1.2%)
NPU operators = 888 (98.8%)

Average SRAM bandwidth                           0.43 GB/s
Input   SRAM bandwidth                           8.30 MB/batch
Weight  SRAM bandwidth                          46.34 MB/batch
Output  SRAM bandwidth                           0.00 MB/batch
Total   SRAM bandwidth                          54.85 MB/batch
Total   SRAM bandwidth            per input     54.85 MB/inference (batch size 1)

Average DRAM bandwidth                           0.91 GB/s
Input   DRAM bandwidth                          69.90 MB/batch
Weight  DRAM bandwidth                           4.76 MB/batch
Output  DRAM bandwidth                          40.12 MB/batch
Total   DRAM bandwidth                         114.77 MB/batch
Total   DRAM bandwidth            per input    114.77 MB/inference (batch size 1)

Neural network macs                        3882797988 MACs/batch
Network Tops/s                                   0.06 Tops/s

NPU cycles                                   53064649 cycles/batch
SRAM Access cycles                            2366448 cycles/batch
DRAM Access cycles                          114460390 cycles/batch
On-chip Flash Access cycles                         0 cycles/batch
Off-chip Flash Access cycles                        0 cycles/batch
Total cycles                                126674229 cycles/batch

Batch Inference time               126.67 ms,    7.89 inferences/s (batch size 1)

